# LLM æœåŠ¡è®¾è®¡

> **æ³¨æ„**: æœ¬æ–‡æ¡£éƒ¨åˆ†å†…å®¹æè¿°äº†ä» LLMClientManager è¿ç§»åˆ° LLMService çš„å†å²è¿‡ç¨‹ï¼Œè¿™äº›æ˜¯è¿ç§»å‰/å†å²è¯´æ˜ï¼Œä¾›å‚è€ƒã€‚å½“å‰å®ç°å·²ä½¿ç”¨ LLMServiceã€‚

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿° LLM è°ƒç”¨èƒ½åŠ›çš„é‡æ„è®¾è®¡ã€‚LLM æœåŠ¡æ˜¯**æ ¸å¿ƒåŸºç¡€è®¾æ–½æœåŠ¡**ï¼Œä¸ EventBus åŒçº§ï¼Œå¯è¢«ä»»ä½• Providerã€æ¨¡å—æˆ–æ’ä»¶ä½¿ç”¨ã€‚

### å…³é”®æ¦‚å¿µæ¾„æ¸…

**LLM æœåŠ¡ä¸æ˜¯ Provider**ã€‚é¡¹ç›®ä¸­çš„ Providerï¼ˆInputProviderã€OutputProviderã€DecisionProviderï¼‰æ˜¯æ•°æ®æµæ¶æ„ä¸­çš„èŠ‚ç‚¹ï¼Œè€Œ LLM æ˜¯è¢«è¿™äº›èŠ‚ç‚¹è°ƒç”¨çš„åŸºç¡€è®¾æ–½æœåŠ¡ã€‚

| æ¦‚å¿µ | é¡¹ç›®ä¸­çš„ Provider | LLM æœåŠ¡ |
|-----|------------------|---------|
| **å®šä½** | æ•°æ®æµæ¶æ„ä¸­çš„èŠ‚ç‚¹ | åŸºç¡€è®¾æ–½æœåŠ¡ |
| **èŒè´£** | å¤„ç†ç‰¹å®šæ•°æ®ç±»å‹çš„è¾“å…¥/è¾“å‡º/å†³ç­– | æä¾› AI èƒ½åŠ›ä¾›å„æ¨¡å—è°ƒç”¨ |
| **æ•°æ®æµ** | æœ‰æ˜ç¡®çš„è¾“å…¥è¾“å‡ºç±»å‹ | æ— ï¼Œæ˜¯å·¥å…· |
| **ç±»æ¯”** | ç®¡é“ä¸­çš„å¤„ç†å™¨ | EventBusã€Logger è¿™ç±»åŸºç¡€æœåŠ¡ |

---

## ğŸ¯ é‡æ„ç›®æ ‡

### å½“å‰é—®é¢˜

| é—®é¢˜ | æè¿° |
|-----|------|
| **æ¶æ„åˆ†è£‚** | `LLMClientManager` + `LLMClient` ä¸ `LocalLLMDecisionProvider` æ˜¯ä¸¤å¥—ç‹¬ç«‹ç³»ç»Ÿ |
| **é…ç½®åˆ†æ•£** | LLMClientManager è¯» config.tomlï¼ŒLocalLLMDecisionProvider æœ‰ç‹¬ç«‹é…ç½®ï¼ŒLLMClient è¿˜æœ‰ global_config å›é€€ |
| **ç¼ºä¹ç»Ÿä¸€æŠ½è±¡** | æ²¡æœ‰ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£ |
| **é”™è¯¯å¤„ç†ä¸ä¸€è‡´** | LocalLLMDecisionProvider æœ‰é‡è¯•/é™çº§ï¼ŒLLMClient åªè¿”å› `success: false` |
| **æµå¼æ”¯æŒä¸ç»Ÿä¸€** | LLMClient æ”¯æŒæµå¼ï¼ŒLocalLLMDecisionProvider ä¸æ”¯æŒ |

### é‡æ„ç›®æ ‡

1. **ç»Ÿä¸€æœåŠ¡æ¥å£**ï¼šæ‰€æœ‰ LLM è°ƒç”¨é€šè¿‡ `LLMService` ç»Ÿä¸€æ¥å£
2. **åç«¯æŠ½è±¡**ï¼šæ”¯æŒå¤šç§ LLM æä¾›å•†ï¼ˆOpenAIã€Ollamaã€Anthropic ç­‰ï¼‰
3. **ç»Ÿä¸€é”™è¯¯å¤„ç†**ï¼šå†…ç½®é‡è¯•ã€è¶…æ—¶ã€é™çº§æœºåˆ¶
4. **é…ç½®é›†ä¸­**ï¼šæ‰€æœ‰ LLM é…ç½®åœ¨ç»Ÿä¸€ä½ç½®
5. **ä¾èµ–æ³¨å…¥**ï¼šé€šè¿‡æ„é€ å‡½æ•°æ³¨å…¥ï¼Œä¾¿äºæµ‹è¯•

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LLM ä½œä¸ºæ ¸å¿ƒæœåŠ¡                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ ¸å¿ƒæœåŠ¡å±‚ (ä¸ EventBus åŒçº§)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  EventBus  â”‚  â”‚ LLMService â”‚  â”‚  Logger    â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚                â”‚
         â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä½¿ç”¨è€… (Provider / æ¨¡å— / æ’ä»¶)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ LocalLLMDecision â”‚  â”‚   AvatarManager  â”‚                    â”‚
â”‚  â”‚    Provider      â”‚  â”‚   (è¡¨æƒ…åˆ†æ)      â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ EmotionJudge     â”‚  â”‚   Maicraft       â”‚                    â”‚
â”‚  â”‚    Plugin        â”‚  â”‚    Plugin        â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç»„ä»¶å±‚æ¬¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLMService (ç»Ÿä¸€æ¥å£)                                          â”‚
â”‚  - chat(), stream_chat(), call_tools(), vision()               â”‚
â”‚  - é…ç½®ç®¡ç†ã€åç«¯é€‰æ‹©ã€é‡è¯•/é™çº§                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLMBackend æŠ½è±¡å±‚                                              â”‚
â”‚  - å®šä¹‰ç»Ÿä¸€çš„åç«¯æ¥å£                                            â”‚
â”‚  - æ¯ä¸ªåç«¯å®ç°ç‰¹å®š API çš„è°ƒç”¨é€»è¾‘                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAIBackend  â”‚ â”‚  OllamaBackend  â”‚ â”‚ AnthropicBackendâ”‚
â”‚  (äº‘ç«¯ API)     â”‚ â”‚  (æœ¬åœ°æ¨¡å‹)     â”‚ â”‚  (Claude API)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ æ¥å£è®¾è®¡

### LLMResponse æ•°æ®ç±»

```python
@dataclass
class LLMResponse:
    """LLM å“åº”ç»“æœ"""
    success: bool
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    usage: Optional[Dict[str, int]] = None
    model: Optional[str] = None
    error: Optional[str] = None
    reasoning_content: Optional[str] = None  # æ¨ç†é“¾å†…å®¹ï¼ˆå¦‚ DeepSeek R1ï¼‰
```

### LLMBackend æŠ½è±¡åŸºç±»

```python
class LLMBackend(ABC):
    """
    LLM åç«¯æŠ½è±¡åŸºç±»
    
    ä¸åŒçš„ LLM æä¾›å•†å®ç°æ­¤æ¥å£ï¼š
    - OpenAIBackend: OpenAI å…¼å®¹ APIï¼ˆåŒ…æ‹¬ SiliconFlowã€DeepSeek ç­‰ï¼‰
    - OllamaBackend: æœ¬åœ° Ollama
    - AnthropicBackend: Claude API
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = get_logger(self.__class__.__name__)
    
    @abstractmethod
    async def chat(
        self,
        messages: List[Dict[str, Any]],
        *,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
    ) -> LLMResponse:
        """èŠå¤©è°ƒç”¨"""
        ...
    
    @abstractmethod
    async def stream_chat(
        self,
        messages: List[Dict[str, Any]],
        *,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stop_event: Optional[asyncio.Event] = None,
    ) -> AsyncIterator[str]:
        """æµå¼èŠå¤©"""
        ...
    
    @abstractmethod
    async def vision(
        self,
        messages: List[Dict[str, Any]],
        images: List[Union[str, bytes]],
        *,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> LLMResponse:
        """è§†è§‰ç†è§£"""
        ...
    
    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æºï¼ˆå­ç±»å¯é€‰é‡å†™ï¼‰"""
        pass
    
    def get_info(self) -> Dict[str, Any]:
        """è·å–åç«¯ä¿¡æ¯"""
        return {
            "name": self.__class__.__name__,
            "model": self.config.get("model"),
            "base_url": self.config.get("base_url"),
        }
```

### LLMService ä¸»ç±»

```python
class LLMService:
    """
    LLM æœåŠ¡ç®¡ç†å™¨
    
    æ ¸å¿ƒåŸºç¡€è®¾æ–½æœåŠ¡ï¼Œä¸ EventBus åŒçº§ã€‚
    
    èŒè´£ï¼š
    - ç®¡ç†å¤šä¸ª LLM åç«¯é…ç½®ï¼ˆllm, llm_fast, vlm ç­‰ï¼‰
    - æä¾›ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
    - å†…ç½®é‡è¯•ã€è¶…æ—¶ã€é™çº§æœºåˆ¶
    - Token ä½¿ç”¨é‡ç»Ÿè®¡
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        ```python
        # åœ¨ AmaidesuCore ä¸­åˆå§‹åŒ–
        self.llm_service = LLMService()
        await self.llm_service.setup(config)
        
        # åœ¨ Provider/æ¨¡å—ä¸­ä½¿ç”¨
        response = await llm_service.chat(
            prompt="ä½ å¥½",
            backend="llm_fast",
        )
        ```
    """
    
    def __init__(self):
        self.logger = get_logger("LLMService")
        self._backends: Dict[str, LLMBackend] = {}
        self._config: Dict[str, Any] = {}
        self._token_manager = TokenUsageManager()
        self._retry_config = RetryConfig()
    
    async def setup(self, config: Dict[str, Any]) -> None:
        """
        ä»é…ç½®åˆå§‹åŒ–æ‰€æœ‰ LLM åç«¯
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸ï¼ŒåŒ…å« [llm], [llm_fast], [vlm] ç­‰éƒ¨åˆ†
        """
        self._config = config
        
        # æ”¯æŒçš„åç«¯ç±»å‹æ˜ å°„
        backend_types = {
            "openai": OpenAIBackend,
            "ollama": OllamaBackend,
            # "anthropic": AnthropicBackend,  # æœªæ¥æ‰©å±•
        }
        
        # åˆå§‹åŒ–é…ç½®ä¸­å®šä¹‰çš„åç«¯
        for name in ["llm", "llm_fast", "vlm"]:
            if name in config:
                backend_config = config[name]
                backend_type = backend_config.get("backend", "openai")
                
                if backend_type not in backend_types:
                    self.logger.warning(f"æœªçŸ¥çš„åç«¯ç±»å‹: {backend_type}ï¼Œä½¿ç”¨ openai")
                    backend_type = "openai"
                
                backend_class = backend_types[backend_type]
                self._backends[name] = backend_class(backend_config)
                self.logger.info(f"å·²åˆå§‹åŒ– {name} åç«¯ ({backend_type})")
    
    # === ä¸»è¦è°ƒç”¨æ¥å£ ===
    
    async def chat(
        self,
        prompt: str,
        *,
        backend: str = "llm",
        system_message: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> LLMResponse:
        """
        èŠå¤©è°ƒç”¨
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            backend: ä½¿ç”¨çš„åç«¯åç§°ï¼ˆllm, llm_fast, vlmï¼‰
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
        
        Returns:
            LLMResponse: å“åº”ç»“æœ
        """
        messages = self._build_messages(prompt, system_message)
        return await self._call_with_retry(
            backend,
            "chat",
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
    
    async def stream_chat(
        self,
        prompt: str,
        *,
        backend: str = "llm",
        system_message: Optional[str] = None,
        stop_event: Optional[asyncio.Event] = None,
    ) -> AsyncIterator[str]:
        """
        æµå¼èŠå¤©è°ƒç”¨
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            backend: ä½¿ç”¨çš„åç«¯åç§°
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            stop_event: åœæ­¢äº‹ä»¶ï¼ˆç”¨äºä¸­æ–­æµå¼è¾“å‡ºï¼‰
        
        Yields:
            str: å¢é‡æ–‡æœ¬å†…å®¹
        """
        llm_backend = self._get_backend(backend)
        messages = self._build_messages(prompt, system_message)
        
        async for chunk in llm_backend.stream_chat(
            messages=messages,
            stop_event=stop_event,
        ):
            yield chunk
    
    async def call_tools(
        self,
        prompt: str,
        tools: List[Dict[str, Any]],
        *,
        backend: str = "llm",
        system_message: Optional[str] = None,
    ) -> LLMResponse:
        """
        å·¥å…·è°ƒç”¨
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆOpenAI æ ¼å¼ï¼‰
            backend: ä½¿ç”¨çš„åç«¯åç§°
            system_message: ç³»ç»Ÿæ¶ˆæ¯
        
        Returns:
            LLMResponse: åŒ…å« tool_calls çš„å“åº”ç»“æœ
        """
        messages = self._build_messages(prompt, system_message)
        return await self._call_with_retry(
            backend,
            "chat",
            messages=messages,
            tools=tools,
        )
    
    async def vision(
        self,
        prompt: str,
        images: List[Union[str, bytes]],
        *,
        backend: str = "vlm",
        system_message: Optional[str] = None,
    ) -> LLMResponse:
        """
        è§†è§‰ç†è§£è°ƒç”¨
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            images: å›¾ç‰‡åˆ—è¡¨ï¼ˆURLã€è·¯å¾„æˆ–å­—èŠ‚ï¼‰
            backend: ä½¿ç”¨çš„åç«¯åç§°ï¼ˆé»˜è®¤ vlmï¼‰
            system_message: ç³»ç»Ÿæ¶ˆæ¯
        
        Returns:
            LLMResponse: å“åº”ç»“æœ
        """
        messages = self._build_messages(prompt, system_message)
        return await self._call_with_retry(
            backend,
            "vision",
            messages=messages,
            images=images,
        )
    
    # === ä¾¿æ·æ–¹æ³• ===
    
    async def simple_chat(
        self,
        prompt: str,
        backend: str = "llm",
        system_message: Optional[str] = None,
    ) -> str:
        """
        ç®€åŒ–èŠå¤©ï¼Œç›´æ¥è¿”å›æ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            backend: ä½¿ç”¨çš„åç«¯åç§°
            system_message: ç³»ç»Ÿæ¶ˆæ¯
        
        Returns:
            str: å“åº”æ–‡æœ¬ï¼Œå¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
        """
        result = await self.chat(prompt, backend=backend, system_message=system_message)
        return result.content if result.success else f"é”™è¯¯: {result.error}"
    
    async def simple_vision(
        self,
        prompt: str,
        images: List[Union[str, bytes]],
        backend: str = "vlm",
    ) -> str:
        """ç®€åŒ–è§†è§‰ç†è§£ï¼Œç›´æ¥è¿”å›æ–‡æœ¬"""
        result = await self.vision(prompt, images, backend=backend)
        return result.content if result.success else f"é”™è¯¯: {result.error}"
    
    # === å†…éƒ¨æ–¹æ³• ===
    
    def _get_backend(self, name: str) -> LLMBackend:
        """è·å–æŒ‡å®šåç«¯"""
        if name not in self._backends:
            raise ValueError(f"LLM åç«¯ '{name}' æœªé…ç½®")
        return self._backends[name]
    
    def _build_messages(
        self,
        prompt: str,
        system_message: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": prompt})
        return messages
    
    async def _call_with_retry(
        self,
        backend_name: str,
        method: str,
        **kwargs,
    ) -> LLMResponse:
        """å¸¦é‡è¯•çš„è°ƒç”¨"""
        llm_backend = self._get_backend(backend_name)
        last_error = None
        
        for attempt in range(self._retry_config.max_retries):
            try:
                method_func = getattr(llm_backend, method)
                result = await method_func(**kwargs)
                
                # è®°å½• token ä½¿ç”¨é‡
                if result.success and result.usage:
                    self._token_manager.record_usage(
                        model_name=result.model or "unknown",
                        prompt_tokens=result.usage.get("prompt_tokens", 0),
                        completion_tokens=result.usage.get("completion_tokens", 0),
                        total_tokens=result.usage.get("total_tokens", 0),
                    )
                
                return result
                
            except Exception as e:
                last_error = e
                self.logger.warning(
                    f"LLM è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self._retry_config.max_retries}): {e}"
                )
                if attempt < self._retry_config.max_retries - 1:
                    delay = min(
                        self._retry_config.base_delay * (2 ** attempt),
                        self._retry_config.max_delay,
                    )
                    await asyncio.sleep(delay)
        
        # æ‰€æœ‰é‡è¯•å¤±è´¥
        self.logger.error(f"æ‰€æœ‰ LLM è°ƒç”¨é‡è¯•å¤±è´¥: {last_error}")
        return LLMResponse(success=False, content=None, error=str(last_error))
    
    # === ç”Ÿå‘½å‘¨æœŸ ===
    
    async def cleanup(self) -> None:
        """æ¸…ç†æ‰€æœ‰åç«¯èµ„æº"""
        for name, backend in self._backends.items():
            try:
                await backend.cleanup()
                self.logger.debug(f"å·²æ¸…ç† {name} åç«¯")
            except Exception as e:
                self.logger.warning(f"æ¸…ç† {name} åç«¯å¤±è´¥: {e}")
        self._backends.clear()
    
    # === ç»Ÿè®¡ä¿¡æ¯ ===
    
    def get_token_usage_summary(self) -> str:
        """è·å– token ä½¿ç”¨é‡æ‘˜è¦"""
        return self._token_manager.format_total_cost_summary()
    
    def get_backend_info(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰åç«¯ä¿¡æ¯"""
        return {name: backend.get_info() for name, backend in self._backends.items()}
```

### RetryConfig é…ç½®ç±»

```python
@dataclass
class RetryConfig:
    """é‡è¯•é…ç½®"""
    max_retries: int = 3
    base_delay: float = 1.0
    max_delay: float = 30.0
```

---

## ğŸ“ ç›®å½•ç»“æ„

```
src/core/
â”œâ”€â”€ llm_service.py              # LLMService ä¸»ç±» + LLMResponse
â”œâ”€â”€ llm_backends/               # åç«¯å®ç°ç›®å½•
â”‚   â”œâ”€â”€ __init__.py             # å¯¼å‡ºæ‰€æœ‰åç«¯
â”‚   â”œâ”€â”€ base.py                 # LLMBackend æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ openai_backend.py       # OpenAI å…¼å®¹ APIï¼ˆä» llm_request.py æ¼”åŒ–ï¼‰
â”‚   â””â”€â”€ ollama_backend.py       # Ollama æœ¬åœ°æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ providers/                  # æ•°æ®æµ Providerï¼ˆä¿æŒä¸å˜ï¼‰
â”‚   â”œâ”€â”€ input_provider.py
â”‚   â”œâ”€â”€ output_provider.py
â”‚   â””â”€â”€ decision_provider.py
â””â”€â”€ ...

# åˆ é™¤çš„æ–‡ä»¶
src/core/llm_client_manager.py  # åˆ é™¤ï¼Œç”± LLMService æ›¿ä»£
src/openai_client/              # é‡æ„ä¸º llm_backends/openai_backend.py
```

---

## âš™ï¸ é…ç½®æ ¼å¼

### æ–°é…ç½®æ ¼å¼

```toml
# config.toml

[llm]
# æ ‡å‡† LLMï¼ˆé«˜è´¨é‡ä»»åŠ¡ï¼‰
backend = "openai"              # openai | ollama | anthropic
model = "deepseek-ai/DeepSeek-V3"
api_key = "your-api-key"
base_url = "https://api.siliconflow.cn/v1/"
temperature = 0.2
max_tokens = 1024
max_retries = 3                 # å¯é€‰ï¼Œé»˜è®¤ 3
retry_delay = 1.0               # å¯é€‰ï¼Œé»˜è®¤ 1.0

[llm_fast]
# å¿«é€Ÿ LLMï¼ˆä½å»¶è¿Ÿä»»åŠ¡ï¼Œå¦‚ Avatar è¡¨æƒ…åˆ†æï¼‰
backend = "openai"
model = "Qwen/Qwen3-8B"
api_key = "your-api-key"
base_url = "https://api.siliconflow.cn/v1/"
temperature = 0.2
max_tokens = 512

[vlm]
# è§†è§‰è¯­è¨€æ¨¡å‹
backend = "openai"
model = "zai-org/GLM-4.6V"
api_key = "your-api-key"
base_url = "https://api.siliconflow.cn/v1/"

# å¯é€‰ï¼šæœ¬åœ° Ollama
[llm_local]
backend = "ollama"
model = "llama3"
api_base = "http://localhost:11434"
api_key = "sk-dummy"            # Ollama ä¸éœ€è¦çœŸå® API key
```

### é…ç½®å˜åŒ–å¯¹æ¯”

| æ—§é…ç½® | æ–°é…ç½® | è¯´æ˜ |
|-------|--------|------|
| `[llm]` | `[llm]` | ä¿æŒä¸å˜ |
| `[llm_fast]` | `[llm_fast]` | ä¿æŒä¸å˜ |
| `[vlm]` | `[vlm]` | ä¿æŒä¸å˜ |
| - | `backend` å­—æ®µ | æ–°å¢ï¼ŒæŒ‡å®šåç«¯ç±»å‹ |
| - | `max_retries` å­—æ®µ | æ–°å¢ï¼Œé‡è¯•æ¬¡æ•° |
| - | `retry_delay` å­—æ®µ | æ–°å¢ï¼Œé‡è¯•å»¶è¿Ÿ |

---

## ğŸ”„ ä½¿ç”¨ç¤ºä¾‹

### åœ¨ AmaidesuCore ä¸­åˆå§‹åŒ–

```python
# src/core/amaidesu_core.py
class AmaidesuCore:
    def __init__(self):
        self.event_bus = EventBus()
        self.llm_service = LLMService()  # ä¸ EventBus åŒçº§
        # ...
    
    async def setup(self, config: Dict[str, Any]) -> None:
        # åˆå§‹åŒ– LLM æœåŠ¡
        await self.llm_service.setup(config)
        # ...
    
    async def cleanup(self) -> None:
        await self.llm_service.cleanup()
        # ...
```

### åœ¨ DecisionProvider ä¸­ä½¿ç”¨

```python
# src/core/providers/local_llm_decision_provider.py
class LocalLLMDecisionProvider(DecisionProvider):
    def __init__(self, config: dict, llm_service: LLMService):
        super().__init__(config)
        self.llm_service = llm_service
    
    async def decide(self, canonical_message: CanonicalMessage) -> MessageBase:
        # ä½¿ç”¨ LLMService è¿›è¡Œå†³ç­–
        response = await self.llm_service.chat(
            prompt=self._build_prompt(canonical_message),
            backend="llm",
            system_message=self.system_prompt,
        )
        
        if not response.success:
            # é™çº§å¤„ç†
            return self._create_fallback_message(canonical_message)
        
        return self._create_message_base(response.content, canonical_message)
```

### åœ¨ AvatarManager ä¸­ä½¿ç”¨

```python
# src/core/avatar/avatar_manager.py
class AvatarManager:
    def __init__(self, llm_service: LLMService, event_bus: EventBus):
        self.llm_service = llm_service
        self.event_bus = event_bus
    
    async def analyze_expression(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬å¹¶ç”Ÿæˆè¡¨æƒ…å‚æ•°"""
        response = await self.llm_service.call_tools(
            prompt=text,
            tools=self.expression_tools,
            backend="llm_fast",  # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹é™ä½å»¶è¿Ÿ
            system_message=self.expression_system_prompt,
        )
        
        if response.success and response.tool_calls:
            return self._parse_tool_calls(response.tool_calls)
        
        return self._default_expression()
```

### åœ¨æ’ä»¶ä¸­ä½¿ç”¨

```python
# src/plugins/emotion_judge/plugin.py
class EmotionJudgePlugin:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.llm_service: Optional[LLMService] = None
    
    async def setup(self, event_bus, config: Dict[str, Any]) -> List[Any]:
        # ä» core è·å– LLMServiceï¼ˆé€šè¿‡ event_bus æˆ–ç›´æ¥æ³¨å…¥ï¼‰
        self.llm_service = config.get("llm_service")
        
        # æ³¨å†Œäº‹ä»¶ç›‘å¬
        event_bus.subscribe("text.received", self._on_text_received)
        return []
    
    async def _on_text_received(self, event_data: Dict[str, Any]) -> None:
        text = event_data.get("text", "")
        
        response = await self.llm_service.chat(
            prompt=f"åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿ: {text}",
            backend="llm_fast",
        )
        
        if response.success:
            await self.event_bus.emit("emotion.analyzed", {
                "text": text,
                "emotion": response.content,
            })
```

---

## ğŸ“‹ å®æ–½æ­¥éª¤

### Phase 1: åˆ›å»ºåŸºç¡€ç»“æ„

1. åˆ›å»º `src/core/llm_service.py`
   - å®šä¹‰ `LLMResponse` æ•°æ®ç±»
   - å®šä¹‰ `RetryConfig` é…ç½®ç±»
   - å®ç° `LLMService` ä¸»ç±»æ¡†æ¶

2. åˆ›å»º `src/core/llm_backends/` ç›®å½•
   - åˆ›å»º `base.py`ï¼š`LLMBackend` æŠ½è±¡åŸºç±»
   - åˆ›å»º `__init__.py`ï¼šå¯¼å‡º

### Phase 2: å®ç° OpenAIBackend

1. å°† `src/openai_client/llm_request.py` ä¸­çš„ `LLMClient` é‡æ„ä¸º `OpenAIBackend`
   - ä¿ç•™æ ¸å¿ƒè°ƒç”¨é€»è¾‘
   - é€‚é… `LLMBackend` æ¥å£
   - ä½¿ç”¨ `git mv` ä¿ç•™å†å²

2. å®ç° `chat()`, `stream_chat()`, `vision()` æ–¹æ³•

### Phase 3: é›†æˆåˆ° AmaidesuCore

1. åœ¨ `AmaidesuCore.__init__()` ä¸­åˆ›å»º `LLMService`
2. åœ¨ `AmaidesuCore.setup()` ä¸­åˆå§‹åŒ– `LLMService`
3. åœ¨ `AmaidesuCore.cleanup()` ä¸­æ¸…ç† `LLMService`

### Phase 4: è¿ç§»ä½¿ç”¨è€…

1. æ›´æ–° `LocalLLMDecisionProvider`ï¼šä½¿ç”¨ `LLMService` è€Œéç›´æ¥è°ƒç”¨ API
2. æ›´æ–° `AvatarManager`ï¼šä½¿ç”¨ `LLMService`
3. æ›´æ–°å…¶ä»–ä½¿ç”¨ LLM çš„æ¨¡å—/æ’ä»¶

### Phase 5: æ¸…ç†

1. åˆ é™¤ `src/core/llm_client_manager.py`
2. åˆ é™¤æˆ–å½’æ¡£ `src/openai_client/` ç›®å½•ä¸­ä¸å†ä½¿ç”¨çš„æ–‡ä»¶
3. æ›´æ–°é…ç½®æ–‡ä»¶æ¨¡æ¿

### Git æ“ä½œç­–ç•¥

| æ“ä½œ | Git å‘½ä»¤ | è¯´æ˜ |
|-----|---------|------|
| é‡å‘½åæ–‡ä»¶ | `git mv old_path new_path` | ä¿ç•™æ–‡ä»¶å†å² |
| ä¿®æ”¹ç°æœ‰æ–‡ä»¶ | ç›´æ¥ç¼–è¾‘ | ä¿ç•™ä¿®æ”¹å†å² |
| åˆ é™¤æ—§æ–‡ä»¶ | `git rm path` | è®°å½•åˆ é™¤å†å² |
| åˆ›å»ºæ–°æ–‡ä»¶ | ç›´æ¥åˆ›å»º | æ–°æ–‡ä»¶ä»æ­¤å¼€å§‹å†å² |

---

## âœ… æˆåŠŸæ ‡å‡†

### åŠŸèƒ½æ ‡å‡†

- [ ] æ‰€æœ‰ç°æœ‰ LLM è°ƒç”¨åŠŸèƒ½æ­£å¸¸è¿è¡Œ
- [ ] æ”¯æŒ `llm`, `llm_fast`, `vlm` ä¸‰ç§é…ç½®
- [ ] æµå¼è¾“å‡ºæ­£å¸¸å·¥ä½œ
- [ ] å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰æ­£å¸¸å·¥ä½œ
- [ ] è§†è§‰ç†è§£ï¼ˆVLMï¼‰æ­£å¸¸å·¥ä½œ
- [ ] Token ä½¿ç”¨é‡ç»Ÿè®¡æ­£å¸¸

### æ¶æ„æ ‡å‡†

- [ ] LLMService ä½œä¸ºæ ¸å¿ƒæœåŠ¡ä¸ EventBus åŒçº§
- [ ] æ‰€æœ‰ LLM è°ƒç”¨é€šè¿‡ `LLMService` ç»Ÿä¸€æ¥å£
- [ ] åç«¯å¯æ‰©å±•ï¼ˆæ”¯æŒæ·»åŠ æ–°çš„ LLMBackendï¼‰
- [ ] é…ç½®é›†ä¸­åœ¨ `config.toml` çš„ `[llm*]` éƒ¨åˆ†
- [ ] é‡è¯•å’Œé™çº§æœºåˆ¶ç»Ÿä¸€

### ä»£ç è´¨é‡æ ‡å‡†

- [ ] åˆ é™¤ `LLMClientManager`
- [ ] åˆ é™¤é‡å¤çš„ LLM è°ƒç”¨ä»£ç 
- [ ] æ‰€æœ‰ LLM ç›¸å…³é…ç½®åœ¨ä¸€å¤„

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [æ¶æ„æ€»è§ˆ](./overview.md) - æ•´ä½“æ¶æ„è®¾è®¡
- [å†³ç­–å±‚è®¾è®¡](./decision_layer.md) - DecisionProvider ç³»ç»Ÿ
- [æ ¸å¿ƒé‡æ„è®¾è®¡](./core_refactoring.md) - AmaidesuCore é‡æ„
