# Amaidesu 配置文件

# === 全局 LLM 配置 ===
# 重要：这是项目通用的 LLM 配置，供所有模块和插件使用
# Avatar 自动表情、插件等功能都会使用这些配置
[llm]
# 标准 LLM 配置（用于高质量任务）
backend = "openai"              # openai | ollama | anthropic
model = "Pro/deepseek-ai/DeepSeek-V3.2"
temperature = 0.2
api_key = "your-api-key"                    # 可选，如果不设置将使用环境变量
base_url = "https://api.siliconflow.cn/v1/" # 可选，用于自定义API端点
max_tokens = 1024
max_retries = 3                 # 可选，默认 3
retry_delay = 1.0               # 可选，默认 1.0

[llm_fast]
# 快速 LLM 配置（用于低延迟任务，如 Avatar 表情分析）
backend = "openai"
model = "Qwen/Qwen3-8B"
temperature = 0.2
api_key = "your-api-key"                    # 可选，如果不设置将使用环境变量
base_url = "https://api.siliconflow.cn/v1/" # 可选，用于自定义API端点
max_tokens = 1024

[vlm]
# 视觉语言模型配置（用于图像理解任务）
backend = "openai"
model = "zai-org/GLM-4.6V"
temperature = 0.3
api_key = "your-api-key"
base_url = "https://api.siliconflow.cn/v1/"
max_tokens = 1024

# 可选：本地 Ollama
[llm_local]
backend = "ollama"
model = "llama3"
api_base = "http://localhost:11434/v1"
api_key = "sk-dummy"            # Ollama 不需要真实 API key

# === Amaidesu 核心配置 ===
[general]
# Amaidesu 在 MaiCore 中注册的平台标识符
platform_id = "amaidesu"

[maicore]
# MaiCore WebSocket 服务器地址
host = "127.0.0.1"
# MaiCore WebSocket 服务器端口
port = 8000
# token = "your_maicore_token_if_needed" # 如果 MaiCore 需要认证，取消注释并设置

[spark_rtasr]
# 讯飞 RTASR 实时识别凭据
# app_id = ""
# access_key_id = ""
# access_key_secret = ""
# 可选：自定义 websocket URL（默认官方地址）
# ws_url = "wss://office-api-ast-dx.iflyaisol.com/ast/communicate/v1"
app_id = ""
access_key_id = ""
access_key_secret = ""


[http_server]
# 是否启用本地 HTTP 回调服务器
enable = true
# 监听的主机地址
host = "127.0.0.1"
# 监听的端口
port = 8080
# MaiCore 或其他服务访问的回调路径
callback_path = "/maicore_callback"

# 上下文管理器配置
[context_manager]
enabled = true

# 上下文格式化配置
[context_manager.formatting]
separator = "\n"           # 上下文项之间的分隔符
add_provider_title = false # 是否在上下文前添加提供者名称
title_separator = ": "     # 如果添加提供者名称，使用的分隔符

# 上下文长度限制配置
[context_manager.limits]
default_max_length = 1000 # 默认上下文最大长度
default_priority = 100    # 默认优先级值

# === 事件总线配置 ===
[event_bus]
# 是否启用事件数据验证（建议仅 debug 模式开启）
# 开启后会验证所有已注册事件的数据格式
enable_validation = false

# ========== 新架构：按层级组织配置 ==========

# === Layer 5: Understanding（表现理解层） ===
[understanding]
# 情感分析配置（唯一位置）
[understanding.emotion_analyzer]
# 情感分析配置（Layer 5 统一入口）
use_rules = true              # 使用规则分析（快速、确定性）
use_llm = false               # 使用 LLM 分析（可选、智能）

# LLM 配置（用于情感分析）
llm_type = "llm_fast"       # 使用全局配置的哪个 LLM
# 自定义 LLM 配置（可选，如果不设置则使用全局配置）
# model = "deepseek-chat"
# temperature = 0.3
# max_tokens = 100

# === Layer 6: Expression（表现生成层） ===
[expression]
# 表情映射配置（统一位置）
# 表情映射自定义（可选覆盖）
[expression.mappings]
# 覆盖默认情感到抽象参数的映射
# happy 开心
[expression.mappings.happy]
smile = 0.9
eye_open = 0.95

# sad 悲伤
[expression.mappings.sad]
smile = -0.3
eye_open = 0.7

# surprised 惊讶
[expression.mappings.surprised]
smile = 0.2
eye_open = 1.0
mouth_open = 0.6

# angry 生气
[expression.mappings.angry]
smile = -0.5
eye_open = 0.6
brow_down = 0.5

# === Layer 7: Rendering（渲染呈现层） ===
[rendering]
# 渲染层配置（已移至 [providers.output]）

# Avatar 输出配置
[rendering.avatar]
enabled = true                    # 是否启用虚拟形象输出
adapter_type = "vts"              # vts | vrchat | live2d

# === Platform Layer（平台抽象层） ===
[platform]
# 平台适配器配置
[platform.vts]
plugin_name = "Amaidesu_VTS_Adapter"
developer = "Amaidesu"
authentication_token_path = "./vts_token.txt"
vts_host = "localhost"
vts_port = 8001

# ========== 旧配置兼容性说明 ==========
# 以下配置节已迁移到新的 [understanding]、[expression]、[platform] 结构
# 但保留以向后兼容。推荐使用新的 [providers.output] 结构
# 旧配置节已标记为过时（deprecated）

# 通用虚拟形象控制系统配置（已迁移到 Platform Layer）
[avatar]
# 已废弃：请使用新的 [rendering.avatar] + [platform] 配置
enabled = false

# 默认活跃适配器（已废弃）
default_adapter = "vts"

# 管道配置
[pipelines]
# === TextPipeline（新架构，Layer 2→3 文本预处理） ===
# TextPipeline 在 NormalizedText → CanonicalMessage 之间执行文本预处理

# 示例：启用限流管道（TextPipeline 版本）
# [pipelines.rate_limit]
# priority = 100        # 数字越小优先级越高
# global_rate_limit = 100  # 可选：全局每分钟最大消息数
# user_rate_limit = 10     # 可选：每个用户每分钟最大消息数
# window_size = 60         # 可选：滑动窗口大小（秒）

# 示例：启用相似文本过滤管道（TextPipeline 版本）
# [pipelines.similar_text_filter]
# priority = 200
# similarity_threshold = 0.85  # 可选：相似度阈值（0-1）
# time_window = 60.0           # 可选：时间窗口（秒）
# max_cache_size = 1000       # 可选：最大缓存条目数

# === MessagePipeline（旧架构，inbound/outbound 消息处理） ===
# MessagePipeline 在 MaiCore 消息的入站/出站路径上处理

# 示例：启用 message_logger 管道
# [pipelines.message_logger]
# priority = 200
# log_dir = "data/message_logs"  # 可选：日志目录

# 示例：启用 command_processor 管道 (入站)
[pipelines.command_processor]
# 这是一个入站管道，用于处理和执行来自 MaiCore 消息中的命令
# 例如，执行 VTubeStudio 热键，然后从文本中移除命令标记
priority = 50
# direction = "inbound" # 此管道的 direction 已在其自己的 config 中设置，此处无需重复
# 可以在此覆盖 command_map
# command_map = { vts_trigger_hotkey = { service = "vts_control", method = "trigger_hotkey" } }

# 插件全局配置
[plugins]
# === 新格式：启用列表 ===
# 取消注释来启用插件，注释掉来禁用
enabled = [
    # 基础功能（建议启用）
    "console_input",
    "keyword_action",
    "llm_text_processor", # LLM文本处理器

    # 游戏互动
    # "mainosaba",        # 魔裁游戏
    # "arknights",        # 明日方舟
    # "minecraft",        # 我的世界

    # 输入功能（语音和文本输入）
    # "stt",              # 语音识别
    # "funasr_stt",       # FunASR语音识别
    # "bili_danmaku",     # B站弹幕输入
    # "bili_danmaku_official",       # B站官方弹幕（不要和bili_danmaku同时启用）
    # "bili_danmaku_official_maicraft",  # B站弹幕-MaiCraft版（不要和bili_danmaku_selenium同时启用）
    # "bili_danmaku_selenium",        # B站弹幕-Selenium版
    # "mock_danmaku",     # 模拟弹幕输入
    # "read_pingmu",      # 读屏木输入
    # "message_replayer", # 消息重放器

    # 输出功能（语音和视觉输出）
    # "tts",              # 语音合成
    # "gptsovits_tts",    # GPT-SoVITS TTS
    # "omni_tts",         # Omni TTS
    # "subtitle",         # 字幕生成
    # "sticker",          # 贴纸生成
    # "emotion_judge",    # 情感判断

    # 外部控制和集成
    # "vtube_studio",     # VTubeStudio控制
    # "warudo",           # Warudo控制
    # "obs_control",      # OBS Studio控制
    # "dg_lab_service",   # DG-Lab服务
    # "dg-lab-do",        # DG-Lab DO
    # "vrchat", # VRChat控制

    # 监控和流媒体
    # "screen_monitor",   # 屏幕监控
    # "remote_stream",    # 远程串流

    # 游戏相关
    # "maicraft",         # MaiCraft游戏

    # 系统工具
    # "command_processor",  # 命令处理器（通常作为管道使用）
]

# === 旧格式（向后兼容，可选） ===
# 以下配置仍然有效，但会被enabled列表覆盖
# enable_console_input = true
# enable_stt = false
# ...（保留原有配置作为注释示例）

# ========== 新增：Phase 5 Provider配置 ==========

# Provider配置统一管理
# 此配置用于统一管理所有Provider（Input、Output、Decision）

# === 输入Provider配置（Layer 1: Perception） ===
[providers.input]
# 是否启用输入层
enabled = true
# 启用的输入Provider列表
inputs = [
    # 控制台输入（开发测试用）
    # "console_input",

    # B站弹幕输入
    # "bili_danmaku",           # 普通B站弹幕
    # "bili_danmaku_official",  # B站官方弹幕（不要和bili_danmaku同时启用）
    # "bili_danmaku_selenium",  # B站弹幕-Selenium版

    # 其他输入
    # "mock_danmaku",           # 模拟弹幕
    # "read_pingmu",            # 读屏木
]

# 各个InputProvider的详细配置
[providers.input.inputs.console_input]
type = "console_input"
# 可选：添加自定义配置

[providers.input.inputs.bili_danmaku]
type = "bili_danmaku"
room_id = ""          # B站直播间号
# ... 其他B站弹幕配置

[providers.input.inputs.mock_danmaku]
type = "mock_danmaku"
# 模拟弹幕配置

# === 输出Provider配置（Layer 7: Rendering） ===
[providers.output]
# 是否启用输出层
enabled = true
# 是否并发渲染到所有Provider
concurrent_rendering = true
# 错误处理策略：continue（继续）、stop（停止）、drop（丢弃）
error_handling = "continue"

# 启用的输出Provider列表
outputs = [
    # 基础输出（推荐启用）
    "subtitle",     # 字幕
    "vts",          # VTS控制

    # TTS输出（选择一个或多个）
    # "tts",         # Edge TTS
    # "omni_tts",    # Omni TTS (GPT-SoVITS)

    # 其他输出
    # "sticker",     # 表情贴纸
]

# ExpressionGenerator配置
[providers.output.expression_generator]
# 默认TTS是否启用
default_tts_enabled = true
# 默认字幕是否启用
default_subtitle_enabled = true
# 默认表情是否启用
default_expressions_enabled = true
# 默认热键是否启用
default_hotkeys_enabled = true

# 各个OutputProvider的详细配置

# TTS Provider配置
[providers.output.outputs.tts]
type = "tts"
# TTS引擎：edge 或 omni
engine = "edge"
# Edge TTS语音
voice = "zh-CN-XiaoxiaoNeural"
# 音频输出设备名称（留空使用默认设备）
output_device_name = ""

# Omni TTS配置（仅在engine=omni时生效）
[providers.output.outputs.tts.omni]
# Omni TTS API地址
api_url = "http://localhost:9880"
# 模型名称
model = "path/to/model"
# 说话人ID
speaker = 0
# 语速
speed = 1.0
# 音调
pitch = 1.0
# 音量
volume = 1.0

# Subtitle Provider配置
[providers.output.outputs.subtitle]
type = "subtitle"
# 字幕窗口宽度
window_width = 800
# 字幕窗口高度
window_height = 100
# 字体大小
font_size = 24
# 字体颜色
font_color = "#FFFFFF"
# 背景颜色
bg_color = "#000000"
# 背景透明度（0-255）
bg_alpha = 180
# 字幕显示位置（top/center/bottom）
position = "bottom"
# 自动隐藏延迟（秒，0表示不隐藏）
auto_hide_delay = 5.0
# 是否启用拖拽
draggable = true
# 是否启用Chroma-key（OBS抠图）
chroma_key_enabled = false
# Chroma-key颜色
chroma_key_color = "#00FF00"

# Sticker Provider配置
[providers.output.outputs.sticker]
type = "sticker"
# 贴纸大小（相对于窗口宽度的比例，0-1）
sticker_size = 0.33
# 贴纸旋转角度（度）
sticker_rotation = 0
# 贴纸显示时长（秒）
display_duration = 3.0
# 贴纸冷却时间（秒，防止刷屏）
cooldown = 5.0
# 贴纸目录
sticker_dir = "data/stickers"
# 默认贴纸（当没有关键词匹配时使用）
default_sticker = "default.png"

# VTS Provider配置
[providers.output.outputs.vts]
type = "vts"
# VTS WebSocket地址
vts_host = "localhost"
vts_port = 8001
# 是否自动连接
auto_connect = true
# 连接超时（秒）
connect_timeout = 10.0
# 重连间隔（秒）
reconnect_interval = 5.0
# 是否启用LLM智能热键匹配
llm_matching_enabled = false
# LLM配置（可选，不设置则使用全局llm配置）
# llm_type = "llm_fast"  # llm, llm_fast, vlm
# max_hotkey_candidates = 5

# Omni TTS Provider配置
[providers.output.outputs.omni_tts]
type = "omni_tts"
# Omni TTS API地址
api_url = "http://localhost:9880"
# 模型路径
model_path = "path/to/model"
# 说话人ID
speaker_id = 0
# 语速
speed = 1.0
# 音调
pitch = 1.0
# 音量
volume = 1.0
# 音频输出设备名称（留空使用默认设备）
output_device_name = ""
# 是否启用流式TTS
stream_enabled = true
# 缓存目录
cache_dir = "data/omni_tts_cache"
# 最大缓存数量（0表示不限制）
max_cache_size = 100

# === 决策Provider配置（Layer 4: Decision） ===
[providers.decision]
# 是否启用决策层
enabled = true
# 当前使用的决策Provider（active provider）
active_provider = "maicore"
# 支持的决策Provider列表
providers = [
    "maicore",        # MaiCore决策（默认）
    "rule_engine",    # 规则引擎
    "local_llm",      # 本地LLM
]

# 各个DecisionProvider的详细配置

# MaiCore决策Provider配置
[providers.decision.providers.maicore]
type = "maicore"
# WebSocket连接配置
host = "127.0.0.1"
port = 8000
# token = "your_token_if_needed"
# 连接超时（秒）
connect_timeout = 10.0
# 重连间隔（秒）
reconnect_interval = 5.0

# 规则引擎决策Provider配置
[providers.decision.providers.rule_engine]
type = "rule_engine"
# 规则文件路径
rules_file = "data/rules/decision_rules.toml"
# 默认回复
default_response = "我不知道怎么回答这个问题"

# 本地LLM决策Provider配置
[providers.decision.providers.local_llm]
type = "local_llm"
# LLM类型（llm, llm_fast, vlm）
llm_type = "llm"
# 自定义模型配置（可选，不设置则使用全局配置）
# model = "gpt-4"
# api_key = "your-api-key"
# base_url = "https://api.openai.com/v1"
# temperature = 0.7
# max_tokens = 1000
# 系统提示词
system_prompt = "你是一个友好的AI助手，请简洁地回答用户的问题。"

# ========== 旧配置兼容性说明 ==========
# 以下配置节已迁移到新的 [providers.*] 结构，但保留以向后兼容
# [rendering] 节点已迁移到 [providers.output]，但仍可使用（已标记为过时）
# 推荐使用新的 [providers.*] 结构，未来版本将移除旧的 [rendering] 节点

# ========== 旧Phase 4渲染层配置（已过时，保留以兼容） ==========
# 注意：此配置节已迁移到 [providers.output]，请使用新配置
[rendering]
# 是否启用输出层
enabled = true
# 是否并发渲染到所有Provider
concurrent_rendering = true
# 错误处理策略：continue（继续）、stop（停止）、drop（丢弃）
error_handling = "continue"

# 启用的输出Provider列表
outputs = [
    # 基础输出（推荐启用）
    "subtitle",     # 字幕
    "vts",          # VTS控制

    # TTS输出（选择一个或多个）
    # "tts",         # Edge TTS
    # "omni_tts",    # Omni TTS (GPT-SoVITS)

    # 其他输出
    # "sticker",     # 表情贴纸
]

# ExpressionGenerator配置
[rendering.expression_generator]
# 默认TTS是否启用
default_tts_enabled = true
# 默认字幕是否启用
default_subtitle_enabled = true
# 默认表情是否启用
default_expressions_enabled = true
# 默认热键是否启用
default_hotkeys_enabled = true

# 各个Provider的详细配置（已迁移到 [providers.output.outputs.xxx]，此处仅作为引用）
[rendering.outputs.tts]
type = "tts"
# 使用 [providers.output.outputs.tts] 的配置

[rendering.outputs.subtitle]
type = "subtitle"
# 使用 [providers.output.outputs.subtitle] 的配置

[rendering.outputs.sticker]
type = "sticker"
# 使用 [providers.output.outputs.sticker] 的配置

[rendering.outputs.vts]
type = "vts"
# 使用 [providers.output.outputs.vts] 的配置

[rendering.outputs.omni_tts]
type = "omni_tts"
# 使用 [providers.output.outputs.omni_tts] 的配置

