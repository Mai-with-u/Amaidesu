"""
PipelineManager å•å…ƒæµ‹è¯•

æµ‹è¯• PipelineManager çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼š
- TextPipeline ç®¡ç†ï¼ˆæ³¨å†Œã€å¤„ç†ã€ä¼˜å…ˆçº§æ’åºï¼‰
- ç»Ÿè®¡ä¿¡æ¯
- é”™è¯¯å¤„ç†
- å¹¶å‘å¤„ç†

è¿è¡Œ: uv run pytest tests/core/test_pipeline_manager.py -v
"""

import asyncio
import pytest
from typing import Optional, Dict, Any

from src.domains.input.pipelines.manager import (
    PipelineManager,
    TextPipelineBase,
    PipelineErrorHandling,
    PipelineStats,
    PipelineException,
    PipelineContext,
)


# =============================================================================
# Mock Pipeline å®ç°
# =============================================================================


class MockTextPipeline(TextPipelineBase):
    """Mock TextPipeline ç”¨äºæµ‹è¯•

    é—®é¢˜#4ä¿®å¤ï¼šæ·»åŠ  context å‚æ•°æ”¯æŒ
    """

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.processed_texts = []
        self.should_drop = False
        self.should_fail = False
        self.should_timeout = False
        self.delay_ms = 0

    async def _process(
        self, text: str, metadata: Dict[str, Any], context: Optional[PipelineContext] = None
    ) -> Optional[str]:
        """å¤„ç†æ–‡æœ¬"""
        self.processed_texts.append((text, metadata))

        if self.delay_ms > 0:
            await asyncio.sleep(self.delay_ms / 1000)

        if self.should_timeout:
            await asyncio.sleep(10)  # è¶…æ—¶

        if self.should_fail:
            raise ValueError("Mock TextPipeline failure")

        if self.should_drop:
            return None

        return f"[{self.__class__.__name__}] {text}"


class SlowMockTextPipeline(TextPipelineBase):
    """æ…¢é€Ÿ Mock TextPipeline ç”¨äºæµ‹è¯•è¶…æ—¶

    é—®é¢˜#4ä¿®å¤ï¼šæ·»åŠ  context å‚æ•°æ”¯æŒ
    """

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.sleep_time = config.get("sleep_time", 1.0)

    async def _process(
        self, text: str, metadata: Dict[str, Any], context: Optional[PipelineContext] = None
    ) -> Optional[str]:
        await asyncio.sleep(self.sleep_time)
        return f"[Slow] {text}"


# =============================================================================
# Test Fixtures
# =============================================================================


@pytest.fixture
def pipeline_manager():
    """åˆ›å»º PipelineManager å®ä¾‹"""
    return PipelineManager()


@pytest.fixture
def sample_text():
    """åˆ›å»ºç¤ºä¾‹æ–‡æœ¬"""
    return "hello world"


@pytest.fixture
def sample_metadata():
    """åˆ›å»ºç¤ºä¾‹å…ƒæ•°æ®"""
    return {"user_id": "test_user", "source": "test"}


# =============================================================================
# TextPipeline æ³¨å†Œæµ‹è¯•
# =============================================================================


@pytest.mark.asyncio
async def test_register_text_pipeline(pipeline_manager: PipelineManager):
    """æµ‹è¯•æ³¨å†Œ TextPipeline"""
    pipeline = MockTextPipeline({})
    pipeline.priority = 100

    pipeline_manager.register_text_pipeline(pipeline)

    assert len(pipeline_manager._text_pipelines) == 1
    assert pipeline_manager._text_pipelines[0] == pipeline
    assert not pipeline_manager._text_pipelines_sorted


@pytest.mark.asyncio
async def test_register_multiple_text_pipelines(pipeline_manager: PipelineManager):
    """æµ‹è¯•æ³¨å†Œå¤šä¸ª TextPipeline"""
    pipeline1 = MockTextPipeline({})
    pipeline2 = MockTextPipeline({})
    pipeline3 = MockTextPipeline({})

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)
    pipeline_manager.register_text_pipeline(pipeline3)

    assert len(pipeline_manager._text_pipelines) == 3


# =============================================================================
# TextPipeline å¤„ç†æµ‹è¯•
# =============================================================================


@pytest.mark.asyncio
async def test_process_text_single_pipeline(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯•å•ä¸ª TextPipeline å¤„ç†æ–‡æœ¬"""
    pipeline = MockTextPipeline({})
    pipeline.priority = 100
    pipeline_manager.register_text_pipeline(pipeline)

    result = await pipeline_manager.process_text(sample_text, sample_metadata)

    assert result is not None
    assert "MockTextPipeline" in result
    assert len(pipeline.processed_texts) == 1
    assert pipeline.processed_texts[0] == (sample_text, sample_metadata)


@pytest.mark.asyncio
async def test_process_text_multiple_pipelines(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯•å¤šä¸ª TextPipeline å¤„ç†æ–‡æœ¬"""
    pipeline1 = MockTextPipeline({})
    pipeline1.priority = 100
    pipeline2 = MockTextPipeline({})
    pipeline2.priority = 50
    pipeline3 = MockTextPipeline({})
    pipeline3.priority = 150

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)
    pipeline_manager.register_text_pipeline(pipeline3)

    await pipeline_manager.process_text(sample_text, sample_metadata)

    # æ‰€æœ‰ç®¡é“éƒ½åº”è¯¥å¤„ç†
    assert len(pipeline1.processed_texts) == 1
    assert len(pipeline2.processed_texts) == 1
    assert len(pipeline3.processed_texts) == 1

    # éªŒè¯å¤„ç†é¡ºåºï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼š50 -> 100 -> 150ï¼‰
    assert pipeline2.processed_texts[0][0] == sample_text
    assert pipeline1.processed_texts[0][0] == f"[MockTextPipeline] {sample_text}"
    assert pipeline3.processed_texts[0][0] == f"[MockTextPipeline] [MockTextPipeline] {sample_text}"


@pytest.mark.asyncio
async def test_process_text_pipeline_drops_message(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯• TextPipeline ä¸¢å¼ƒæ¶ˆæ¯"""
    pipeline1 = MockTextPipeline({})
    pipeline1.priority = 100
    pipeline2 = MockTextPipeline({})
    pipeline2.priority = 200
    pipeline2.should_drop = True

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)

    result = await pipeline_manager.process_text(sample_text, sample_metadata)

    # pipeline2 ä¸¢å¼ƒæ¶ˆæ¯ï¼Œåº”è¿”å› None
    assert result is None
    assert len(pipeline1.processed_texts) == 1
    assert len(pipeline2.processed_texts) == 1

    # éªŒè¯ä¸¢å¼ƒè®¡æ•°
    stats2 = pipeline2.get_stats()
    assert stats2.dropped_count == 1


@pytest.mark.asyncio
async def test_process_text_empty_pipeline_list(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯•ç©º TextPipeline åˆ—è¡¨"""
    result = await pipeline_manager.process_text(sample_text, sample_metadata)

    assert result == sample_text


@pytest.mark.asyncio
async def test_process_text_disabled_pipeline(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯•ç¦ç”¨çš„ TextPipeline ä¸å¤„ç†"""
    pipeline1 = MockTextPipeline({})
    pipeline1.priority = 100
    pipeline2 = MockTextPipeline({})
    pipeline2.priority = 200
    pipeline2.enabled = False

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)

    await pipeline_manager.process_text(sample_text, sample_metadata)

    # åªæœ‰ pipeline1 å¤„ç†äº†
    assert len(pipeline1.processed_texts) == 1
    assert len(pipeline2.processed_texts) == 0


# =============================================================================
# TextPipeline é”™è¯¯å¤„ç†æµ‹è¯•
# =============================================================================


@pytest.mark.asyncio
async def test_process_text_pipeline_error_continue(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯• TextPipeline é”™è¯¯å¤„ç†ï¼šCONTINUE"""
    pipeline1 = MockTextPipeline({})
    pipeline1.priority = 100
    pipeline2 = MockTextPipeline({})
    pipeline2.priority = 200
    pipeline2.should_fail = True
    pipeline2.error_handling = PipelineErrorHandling.CONTINUE

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)

    result = await pipeline_manager.process_text(sample_text, sample_metadata)

    # pipeline2 å¤±è´¥ä½†ç»§ç»­ï¼Œpipeline1 åº”è¯¥å·²å¤„ç†
    assert result is not None
    assert len(pipeline1.processed_texts) == 1
    assert len(pipeline2.processed_texts) == 1

    # éªŒè¯é”™è¯¯è®¡æ•°ï¼ˆåœ¨ process() ä¸­ incrementï¼Œåœ¨é”™è¯¯å¤„ç†ä¸­å†æ¬¡ incrementï¼‰
    stats2 = pipeline2.get_stats()
    assert stats2.error_count >= 1  # è‡³å°‘ä¸€æ¬¡ï¼Œå¯èƒ½ä¸¤æ¬¡ï¼ˆå–å†³äºå®ç°ï¼‰


@pytest.mark.asyncio
async def test_process_text_pipeline_error_stop(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯• TextPipeline é”™è¯¯å¤„ç†ï¼šSTOP"""
    pipeline = MockTextPipeline({})
    pipeline.priority = 100
    pipeline.should_fail = True
    pipeline.error_handling = PipelineErrorHandling.STOP

    pipeline_manager.register_text_pipeline(pipeline)

    with pytest.raises(PipelineException) as exc_info:
        await pipeline_manager.process_text(sample_text, sample_metadata)

    assert "MockTextPipeline" in str(exc_info.value)
    assert "å¤„ç†å¤±è´¥" in str(exc_info.value)


@pytest.mark.asyncio
async def test_process_text_pipeline_error_drop(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯• TextPipeline é”™è¯¯å¤„ç†ï¼šDROP"""
    pipeline1 = MockTextPipeline({})
    pipeline1.priority = 100
    pipeline2 = MockTextPipeline({})
    pipeline2.priority = 200
    pipeline2.should_fail = True
    pipeline2.error_handling = PipelineErrorHandling.DROP

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)

    result = await pipeline_manager.process_text(sample_text, sample_metadata)

    # pipeline2 å¤±è´¥å¹¶ä¸¢å¼ƒæ¶ˆæ¯
    assert result is None

    # éªŒè¯é”™è¯¯å’Œä¸¢å¼ƒè®¡æ•°ï¼ˆerror_count å¯èƒ½ increment ä¸¤æ¬¡ï¼‰
    stats2 = pipeline2.get_stats()
    assert stats2.error_count >= 1
    assert stats2.dropped_count == 1


@pytest.mark.asyncio
async def test_process_text_pipeline_timeout_continue(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯• TextPipeline è¶…æ—¶ï¼šCONTINUE"""
    pipeline1 = MockTextPipeline({})
    pipeline1.priority = 100
    pipeline2 = SlowMockTextPipeline({"sleep_time": 1.0, "timeout_seconds": 0.1})
    pipeline2.priority = 200
    pipeline2.timeout_seconds = 0.1
    pipeline2.error_handling = PipelineErrorHandling.CONTINUE

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)

    result = await pipeline_manager.process_text(sample_text, sample_metadata)

    # pipeline2 è¶…æ—¶ä½†ç»§ç»­ï¼Œpipeline1 åº”è¯¥å·²å¤„ç†
    assert result is not None

    # éªŒè¯é”™è¯¯è®¡æ•°
    stats2 = pipeline2.get_stats()
    assert stats2.error_count == 1


@pytest.mark.asyncio
async def test_process_text_pipeline_timeout_drop(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯• TextPipeline è¶…æ—¶ï¼šDROP"""
    pipeline1 = MockTextPipeline({})
    pipeline1.priority = 100
    pipeline2 = SlowMockTextPipeline({"sleep_time": 1.0})
    pipeline2.priority = 200
    pipeline2.timeout_seconds = 0.1
    pipeline2.error_handling = PipelineErrorHandling.DROP

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)

    result = await pipeline_manager.process_text(sample_text, sample_metadata)

    # pipeline2 è¶…æ—¶å¹¶ä¸¢å¼ƒ
    assert result is None

    # éªŒè¯é”™è¯¯å’Œä¸¢å¼ƒè®¡æ•°
    stats2 = pipeline2.get_stats()
    assert stats2.error_count == 1
    assert stats2.dropped_count == 1


# =============================================================================
# TextPipeline ä¼˜å…ˆçº§æ’åºæµ‹è¯•
# =============================================================================


@pytest.mark.asyncio
async def test_text_pipeline_priority_sorting(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯• TextPipeline æŒ‰ä¼˜å…ˆçº§æ’åº"""
    execution_order = []

    class OrderedTextPipeline(MockTextPipeline):
        def __init__(self, config: Dict[str, Any], name: str):
            super().__init__(config)
            self.name = name

        async def _process(
            self, text: str, metadata: Dict[str, Any], context: Optional[PipelineContext] = None
        ) -> Optional[str]:
            execution_order.append(self.name)
            return f"[{self.name}] {text}"

    pipeline1 = OrderedTextPipeline({}, "pipeline1")
    pipeline1.priority = 100
    pipeline2 = OrderedTextPipeline({}, "pipeline2")
    pipeline2.priority = 50
    pipeline3 = OrderedTextPipeline({}, "pipeline3")
    pipeline3.priority = 150

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)
    pipeline_manager.register_text_pipeline(pipeline3)

    await pipeline_manager.process_text(sample_text, sample_metadata)

    # éªŒè¯æ‰§è¡Œé¡ºåºï¼š50 -> 100 -> 150
    assert execution_order == ["pipeline2", "pipeline1", "pipeline3"]


# =============================================================================
# ç»Ÿè®¡ä¿¡æ¯æµ‹è¯•
# =============================================================================


@pytest.mark.asyncio
async def test_get_text_pipeline_stats(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯•è·å– TextPipeline ç»Ÿè®¡ä¿¡æ¯"""
    pipeline1 = MockTextPipeline({})
    pipeline1.priority = 100
    pipeline2 = MockTextPipeline({})
    pipeline2.priority = 200
    pipeline2.enabled = False

    pipeline_manager.register_text_pipeline(pipeline1)
    pipeline_manager.register_text_pipeline(pipeline2)

    # å¤„ç†ä¸€äº›æ–‡æœ¬ï¼ˆåªæœ‰ pipeline1 ä¼šå¤„ç†ï¼Œpipeline2 è¢«ç¦ç”¨ï¼‰
    await pipeline_manager.process_text(sample_text, sample_metadata)
    await pipeline_manager.process_text(sample_text, sample_metadata)

    stats = pipeline_manager.get_text_pipeline_stats()

    # ä¸¤ä¸ª pipeline éƒ½æœ‰ç»Ÿè®¡ï¼Œä½†ç”±äºåŒåï¼Œåæ³¨å†Œçš„ä¼šè¦†ç›–ï¼ˆè¿”å›æœ€åä¸€ä¸ªï¼‰
    assert len(stats) >= 1
    assert "MockTextPipeline" in stats
    # ç»Ÿè®¡æ¥è‡ªæœ€åæ³¨å†Œçš„ pipeline2ï¼ˆdisabledï¼‰
    # æˆ–æ¥è‡ª pipeline1ï¼ˆprocessed 2æ¬¡ï¼‰ï¼Œå–å†³äºå®ç°
    # éªŒè¯åŸºæœ¬ç»“æ„
    assert "processed_count" in stats["MockTextPipeline"]
    assert "dropped_count" in stats["MockTextPipeline"]
    assert "error_count" in stats["MockTextPipeline"]
    assert "enabled" in stats["MockTextPipeline"]
    assert "priority" in stats["MockTextPipeline"]


@pytest.mark.asyncio
async def test_get_text_pipeline_stats_with_drops(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯åŒ…å«ä¸¢å¼ƒè®¡æ•°"""
    pipeline = MockTextPipeline({})
    pipeline.priority = 100
    pipeline.should_drop = True

    pipeline_manager.register_text_pipeline(pipeline)

    await pipeline_manager.process_text(sample_text, sample_metadata)

    stats = pipeline_manager.get_text_pipeline_stats()

    assert stats["MockTextPipeline"]["dropped_count"] == 1


@pytest.mark.asyncio
async def test_get_text_pipeline_stats_with_errors(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯åŒ…å«é”™è¯¯è®¡æ•°"""
    pipeline = MockTextPipeline({})
    pipeline.priority = 100
    pipeline.should_fail = True
    pipeline.error_handling = PipelineErrorHandling.CONTINUE

    pipeline_manager.register_text_pipeline(pipeline)

    await pipeline_manager.process_text(sample_text, sample_metadata)

    stats = pipeline_manager.get_text_pipeline_stats()

    # error_count å¯èƒ½ increment ä¸¤æ¬¡ï¼ˆåœ¨ process() å’Œé”™è¯¯å¤„ç†ä¸­ï¼‰
    assert stats["MockTextPipeline"]["error_count"] >= 1


@pytest.mark.asyncio
async def test_get_text_pipeline_stats_avg_duration(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯åŒ…å«å¹³å‡å¤„ç†æ—¶é—´"""
    pipeline = MockTextPipeline({})
    pipeline.priority = 100
    pipeline.delay_ms = 50  # 50ms å»¶è¿Ÿ

    pipeline_manager.register_text_pipeline(pipeline)

    await pipeline_manager.process_text(sample_text, sample_metadata)
    await pipeline_manager.process_text(sample_text, sample_metadata)

    stats = pipeline_manager.get_text_pipeline_stats()

    assert stats["MockTextPipeline"]["processed_count"] == 2
    assert stats["MockTextPipeline"]["avg_duration_ms"] >= 50  # è‡³å°‘ 50ms


@pytest.mark.asyncio
async def test_get_text_pipeline_stats_empty(pipeline_manager: PipelineManager):
    """æµ‹è¯•ç©º TextPipeline åˆ—è¡¨çš„ç»Ÿè®¡"""
    stats = pipeline_manager.get_text_pipeline_stats()

    assert len(stats) == 0


# =============================================================================
# PipelineStats æµ‹è¯•
# =============================================================================


def test_pipeline_stats_defaults():
    """æµ‹è¯• PipelineStats é»˜è®¤å€¼"""
    stats = PipelineStats()

    assert stats.processed_count == 0
    assert stats.dropped_count == 0
    assert stats.error_count == 0
    assert stats.total_duration_ms == 0
    assert stats.avg_duration_ms == 0


def test_pipeline_stats_avg_duration_no_processed():
    """æµ‹è¯•æ²¡æœ‰å¤„ç†è®°å½•æ—¶çš„å¹³å‡æ—¶é•¿"""
    stats = PipelineStats()

    assert stats.avg_duration_ms == 0


def test_pipeline_stats_avg_duration_with_processed():
    """æµ‹è¯•è®¡ç®—å¹³å‡å¤„ç†æ—¶é•¿"""
    stats = PipelineStats()
    stats.processed_count = 3
    stats.total_duration_ms = 150

    assert stats.avg_duration_ms == 50


# =============================================================================
# å¹¶å‘æµ‹è¯•
# =============================================================================


@pytest.mark.asyncio
async def test_concurrent_text_processing(
    pipeline_manager: PipelineManager, sample_text: str, sample_metadata: Dict[str, Any]
):
    """æµ‹è¯•å¹¶å‘å¤„ç†å¤šä¸ªæ–‡æœ¬"""
    pipeline = MockTextPipeline({})
    pipeline.priority = 100
    pipeline.delay_ms = 10

    pipeline_manager.register_text_pipeline(pipeline)

    # å¹¶å‘å¤„ç† 10 ä¸ªæ–‡æœ¬
    tasks = [pipeline_manager.process_text(f"text_{i}", sample_metadata) for i in range(10)]

    results = await asyncio.gather(*tasks)

    # æ‰€æœ‰æ–‡æœ¬éƒ½åº”è¯¥è¢«å¤„ç†
    assert len(pipeline.processed_texts) == 10
    assert all(r is not None for r in results)


# =============================================================================
# è¾¹ç•Œæƒ…å†µæµ‹è¯•
# =============================================================================


@pytest.mark.asyncio
async def test_process_text_empty_string(pipeline_manager: PipelineManager, sample_metadata: Dict[str, Any]):
    """æµ‹è¯•å¤„ç†ç©ºå­—ç¬¦ä¸²"""
    pipeline = MockTextPipeline({})
    pipeline.priority = 100

    pipeline_manager.register_text_pipeline(pipeline)

    result = await pipeline_manager.process_text("", sample_metadata)

    assert result is not None
    assert len(pipeline.processed_texts) == 1
    assert pipeline.processed_texts[0][0] == ""


@pytest.mark.asyncio
async def test_process_text_with_unicode(pipeline_manager: PipelineManager, sample_metadata: Dict[str, Any]):
    """æµ‹è¯•å¤„ç† Unicode æ–‡æœ¬"""
    pipeline = MockTextPipeline({})
    pipeline.priority = 100

    pipeline_manager.register_text_pipeline(pipeline)

    unicode_text = "ä½ å¥½ä¸–ç•Œ ğŸŒ Ã‘oÃ±o"
    result = await pipeline_manager.process_text(unicode_text, sample_metadata)

    assert result is not None
    assert "ä½ å¥½ä¸–ç•Œ" in result or "MockTextPipeline" in result


# =============================================================================
# è¿è¡Œå…¥å£
# =============================================================================


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
